# Setup of GCP components including KMS keys for building CMEK protected Streaming Pipeline
# Additionally make use of geo-fencing to ensure data is processed in one (known) regional
# "str-pl" --> streaming pipeline

# Project level metadata

PROJECT_ID=data-analytics-bk
PROJECT_NUMBER=388488735436
REGION=europe-west2
TOPIC_NAME=str-pl-pubsub-topic
DATASET_NAME=da_streaming_pipeline
TABLE_NAME_1=cc_account_info
TABLE_NAME_2=cc_count_per_country
STAGING_LOCATION=gs://da_streaming_pipeline/stage
TEMP_LOCATION=gs://da_streaming_pipeline/temp

export PROJECT_ID PROJECT_NUMBER REGION TOPIC_NAME DATASET_NAME TABLE_NAME_1 TABLE_NAME_2 STAGING_LOCATION TEMP_LOCATION


# Create a CMEK key in London which should be used by PubSub and BigQuery.

  0. Create a Key Ring (str-pl-london-kr) in London region (europe-west2)

     gcloud kms keyrings create str-pl-london-kr --location ${REGION}

  1. Create a key (str-pl-london-key01) in Key Ring (str-pl-london-kr)

     gcloud kms keys create str-pl-london-key01 --location ${REGION} --keyring str-pl-london-kr --purpose encryption

# Create a Global CMEK key which should be used by Dataflow. Global key is required because Dataflow endpoint is in europe-west1
  but the worker nodes should be in London

  0. Create a Global Key Ring (str-pl-global-kr)

     gcloud kms keyrings create str-pl-global-kr --location global

  1. Create a key (str-pl-global-key01) in Key Ring (str-pl-global-kr)

     gcloud kms keys create str-pl-global-key01 --location global --keyring str-pl-global-kr --purpose encryption



# Grant permission to encrypt and decrypt data using CMEK to service accounts for PubSub, bigquery and Dataflow

  0. Grant permission to PubSub service account

     gcloud projects add-iam-policy-binding ${PROJECT_ID} \
     --member="serviceAccount:service-${PROJECT_NUMBER}@gcp-sa-pubsub.iam.gserviceaccount.com" \
     --role='roles/cloudkms.cryptoKeyEncrypterDecrypter'

  1. Grant permission to BigQuery service accounts

     gcloud kms keys add-iam-policy-binding \
     --project=data-analytics-bk \
     --member serviceAccount:bq-388488735436@bigquery-encryption.iam.gserviceaccount.com \
     --role roles/cloudkms.cryptoKeyEncrypterDecrypter \
     --location=${REGION} \
     --keyring=str-pl-london-kr \
     str-pl-london-key01

  2. Grant permission to Dataflow and Compute engine service accounts as required by Dataflow.

     gcloud projects add-iam-policy-binding ${PROJECT_ID} \
     --member serviceAccount:service-${PROJECT_NUMBER}@dataflow-service-producer-prod.iam.gserviceaccount.com \
     --role roles/cloudkms.cryptoKeyEncrypterDecrypter

     gcloud projects add-iam-policy-binding ${PROJECT_ID} \
     --member serviceAccount:service-${PROJECT_NUMBER}@compute-system.iam.gserviceaccount.com \
     --role roles/cloudkms.cryptoKeyEncrypterDecrypter


# Set up PubSub Topic

  0. Build KEY Resource ID

     KEY_ID=projects/${PROJECT_ID}/locations/${REGION}/keyRings/str-pl-london-kr/cryptoKeys/str-pl-london-key01
     export KEY_ID

  1. Create PubSub Topic

     gcloud pubsub topics create ${TOPIC_NAME} \
     --message-storage-policy-allowed-regions=${REGION} --topic-encryption-key=$KEY_ID

  2. Confirm that the key has been created using

     gcloud pubsub topics describe ${TOPIC_NAME}


# Set up BigQuery Dataset and Table

  0. Create a dataset (da_streaming_pipeline) in London region (NOTE - The bq command accepts the KMS_KEY but the resulting dataset is
     not protected using CMEK. Raised a bug - b/146544447. In the meantime, create Dataset using UI which works fine with CMEK keys)

     bq --location=${REGION} mk --dataset \
     --description "dataset for streaming pipeline sink" \
     --destination_kms_key ${KEY_ID} \
     ${PROJECT_ID}:${DATASET_NAME}

  1. Create Sink TABLE for Raw data

    bq query --use_legacy_sql=false "
      CREATE TABLE ${DATASET_NAME}.${TABLE_NAME_1}
          (
            name STRING,
            address STRING,
            country_code STRING,
            city STRING,
            bank_iban STRING,
            company STRING,
            credit_card INT64,
            credit_card_provider STRING,
            credit_card_expires STRING,
            date_record_created DATETIME,
            employement STRING,
            emp_id STRING,
            name_prefix STRING,
            phone_number STRING
          )
      OPTIONS(
         description='Sink table to collect data after processing by dataflow for streaming pipeline'
      )
   "

  2. Create Sink Table for aggregated data (count of CC per country in a fixed window)

    bq query --use_legacy_sql=false "
      CREATE TABLE ${DATASET_NAME}.${TABLE_NAME_2}
           (
            country_name STRING,
            num_cc_card  INT64
           )
      OPTIONS(
          description='Sink table to collect aggregated data per fixed window'
      )"


# Setup CMEK protected GCS buckets for Staging and Temp locations

    0.  gsutil mb -c regional -l europe-west2 gs://da_streaming_pipeline

    1.  gsutil kms encryption \
        -k projects/${PROJECT_ID}/locations/${REGION}/keyRings/str-pl-london-kr/cryptoKeys/str-pl-london-key01 \
        gs://da_streaming_pipeline

# Generate Data and ingest to PubSub TOPIC_NAME

(code in /Users/bipulk/gcp/da/datagen)

./datagen.py -d pubsub -p data-analytics-bk -t str-pl-pubsub-topic -n 100000

# Start Data processing pipeline using Dataflow runner

./streaming_df.py --project_name data-analytics-bk --pubsub_topic projects/data-analytics-bk/topics/str-pl-pubsub-topic --streaming

python -m streaming_df \
  --project_name data-analytics-bk \
  --pubsub_topic projects/data-analytics-bk/topics/str-pl-pubsub-topic \
  --streaming \
  --runner Dataflow \
  --staging_location gs://da_streaming_pipeline/stage \
  --temp_location gs://da_streaming_pipeline/temp \
  --region europe-west1 --zone=europe-west2-b \
  --dataflow_kms_key projects/data-analytics-bk/locations/global/keyRings/str-pl-global-kr/cryptoKeys/str-pl-global-key01


# Error encountered and the solutions

0. version of Python package "six" was old. Update with the following command

  check version of python package six

  pip freeze | grep six

  Update to 1.12.0

  pip install six==1.12.0 --user

1. Failed with write permission error on the staging Bucket

IOError: Could not upload to GCS path gs://da_streaming_pipeline/stage/strplccprocessing.1576771784.505801: bucket not found.
Please verify that credentials are valid and that you have write access to the specified path.

Q. -> Which service account is used to run the pipeline ?

bipulk-macbookpro:pipeline bipulk$ gcloud config list
[compute]
zone = us-central1-a
[core]
account = da-bk-2019@data-analytics-bk.iam.gserviceaccount.com
disable_usage_reporting = False
project = data-analytics-bk

Grant Storage Admin, Dataflow Admin, Pubsub and BigQuery related roles to Service Account.
Also enable Dataflow API.
